{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux Antagonistes Génératifs (GANs)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Les Réseaux Antagonistes Génératifs (GANs) sont composés de deux réseaux neuronaux qui s'affrontent dans un processus d'apprentissage concurrentiel :\n",
    "\n",
    "- **Le générateur** : produit des données pour tromper le discriminateur.\n",
    "- **Le discriminateur** : évalue les données et tente de distinguer les fausses des vraies.\n",
    "\n",
    "Ces deux réseaux s'entraînent simultanément :\n",
    "\n",
    "- Le générateur améliore sa capacité à produire des données réalistes.\n",
    "- Le discriminateur affine sa capacité à détecter les fausses données.\n",
    "\n",
    "## 2. Fonctionnement\n",
    "\n",
    "Le processus d'entraînement consiste à mélanger les fausses données générées par le générateur avec de vraies données et les transmettre au discriminateur.  \n",
    "Ce dernier, étant un classificateur binaire, attribue :\n",
    "\n",
    "- **1** : si l'image est réelle.\n",
    "- **0** : si l'image est fausse.\n",
    "\n",
    "\n",
    "## 3. Architecture des réseaux\n",
    "\n",
    "### 3.1 Générateur\n",
    "\n",
    "**Entrée** : Bruit aléatoire (vecteur de dimension **100×1**, suivant une loi normale centrée réduite).\n",
    "\n",
    "**Couches** :\n",
    "\n",
    "1. **Dense layer + Reshape** avec des convolutions:\n",
    "   - Transformation du vecteur **100×1** en un tenseur **8×8×128**.\n",
    "   - Opération :  $ y = Wz + b $ avec  W  de taille **(8192,100)**.\n",
    "   - Activation : **ReLU** (remplace les valeurs négatives par 0).\n",
    "   - Reshape : **8192×1** en **(8,8,128)**.\n",
    "\n",
    "2. **Conv2D Transpose (Upsampling)** avec des convolutions  :\n",
    "   - **(8,8,128) → (16,16,256)**\n",
    "   - **(16,16,256) → (32,32,128)**\n",
    "   - **(32,32,128) → (64,64,64)**\n",
    "\n",
    "3. **Conv2D (Finale)** :\n",
    "   - Transformation finale en **(64,64,3)** pour obtenir une image **RGB**.\n",
    "\n",
    "**Sortie** : Image générée **G(z)**.\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Discriminateur\n",
    "\n",
    "**Entrée** : Images réelles $x$ et images générées $ G(z)$.\n",
    "\n",
    "**Fonction** : Classificateur binaire (renvoie une probabilité d'appartenance de l'entrée aux données réelles).\n",
    "\n",
    "\n",
    "\n",
    "## 4. Fonction de Coût\n",
    "\n",
    "### 4.1 Générateur $G_\\theta$\n",
    "\n",
    "**Objectif** : Faire croire au discriminateur que l'image générée est réelle.\n",
    "\n",
    "$L_g = max_{\\theta} E_{z \\sim N(0,1)}\\left[\\log(D_{\\theta '}(G_{\\theta}(z)))\\right]$\n",
    "\n",
    "\n",
    "- Si **$D_{\\theta '}(G(z))$ = 0.9**, le discriminateur pense que l'image est réelle → le générateur trompe le discriminateur\n",
    "- Si **$D_{\\theta '}(G(z))$ = 0.5**, le générateur a besoin de plus d'entraînement\n",
    "\n",
    "\n",
    "### 4.2 Discriminateur $D_{\\theta '}$\n",
    "\n",
    "**Objectif** : Distinguer les vraies images des fausses.\n",
    "\n",
    "$\n",
    "L_d = \\max_{\\theta'} E_{x \\sim p_{\\text{data}}} \\left[\\log D_{\\theta'}(x)\\right] + E_{z \\sim N(0,1)} \\left[\\log (1 - D_{\\theta'}(G_{\\theta}(z)))\\right]\n",
    "$ . \n",
    "\n",
    "\n",
    "- **D(x) → 1** pour les images réelles.\n",
    "- **D(G(z)) → 0** pour les images générées.\n",
    "\n",
    "\n",
    "## 5. Descente de gradient\n",
    "\n",
    "Selon le feedback du discriminateur on a recours à la descente de gradient suivant le calcul des fonctions $L_d$ et $L_g$. Ainsi :   \n",
    "$ \\theta (t+1) = \\theta (t) + \\alpha \\frac{1}{N}\\partial_{\\theta} \\sum_{i=1}^N \\log{(D_{\\theta}(G_{\\theta}(z^i(t)))}$  \n",
    "$\\theta' (t+1) = \\theta' (t) + \\alpha \\frac{1}{N}\\partial_{\\theta'} \\sum_{i=1}^{N}( \\log(D_{\\theta'}(x^i)) + \\log( 1-  D_{\\theta'}(G_{\\theta}(z^i(t)))))$\n",
    "\n",
    "Ces formules se basent sur la loi des grand nombres qui donne une bonne approximation de l'espérance. Cette dernière nécessitant une infinité d'échantillons, on est obligés ainsi de passer par la loi des grands nombres dans la formule des gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
